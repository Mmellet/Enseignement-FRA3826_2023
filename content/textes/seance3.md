#### Séance 3 : Qu’est-ce que l’édition numérique ? (volet 2 : Architectures numériques)

On avait établi que la définition de l'édition au vu de l'évolution des pratiques culturelles est un ensemble complexe qui ne peut pas se résumer à un idéal de communication : elle se complexifie avec des dimensions économiques et politiques qui viennent épaissir encore davantage le statut d'éditeur. À la suite de l'analyse de Goody sur la structuration de l'écriture, il est également apparu que pour saisir les enjeux culturels de l'édition, et plus largement l'organisation d'une écriture savante, il ne faut pas en faire une abstraction mais bien s'intéresser à son support d'inscription. Le geste d'édition comme la lecture chez Christin, dont la théorie vient se poser contre la raison graphique de Goddy, est une *invention du support* au sens où c'est ce qui va permettre d'agencer la matière tout en dépendant de ses caractéristiques physiques (une inscription sur pierre, bois, peau ne s'établit pas de la même manière et n'offre pas les mêmes propritétés en terme de lisibilité, pérennité, plasticités). L'édition numérique étant l'implémentation, mais aussi la redéfinition, de gestes et d'objectifs d'édition dans un nouvel ensemble médiatique, cette troisième et dernière séance introductive sera consacrée aux architectures numériques. 

Cette séance s'intéressera sur trois marqueurs importants du numérique comme outil mais aussi comme instance médiatique : 

- le principe de numérique ; 
- Internet ; 
- le Web ; 
- quel nom pour l'édition numérique avec l'analyse de l'article de Dacos et Mounier.

Ces fondamentaux seront repris et réapprofondis dans les séances qui suivront (autour de la question des formats, de l'hypertexte etc.).

Pour entrer dans les architectures numériques, il nous faut d'abord nous demander de quoi « numérique » est le nom. 

## Le numérique avant le numérique 

### Numérique / Analogique 

Traditionnellement opposé à l'analogique, le numérique est un modèle de représentation et de structuration de l'information. 

Quelle est la différence entre les technologies suivantes ? 

- une montre à cadran
- une imprimante
- une appareil photo argentique
- un gramophone
- une télévision

Au-delà de leur différence d'usage, des différentes adresses sensorielles que ces objets convoquent, on les distingue surtout selon le modèle de représentation du monde qu'ils incarnent : modèle analogique ou modèle numérique. 

{{< columns >}}

#### Analogique

{{< hint info >}}

*Définition*: Le modèle analogique se fonde sur le principe d'analogie, ce qu'il veut dire que pour représenter une réalité physique complexe, le modèle va utiliser une unité de mesure différente mais ressemblante (analogue). Dans le modèle analogique, la donnée est continue donc le signal analogique est continu comme une onde.

{{< /hint >}}

> L’enregistrement analogique se base sur une reproduction du son de façon analogue à la réalité, à savoir en reproduisant sur un support -- par exemple un vinyle -- la continuité de l’onde sonore. Concrètement, la pointe du dispositif d’enregistrement est ébranlée par le son et reproduit un mouvement analogue à celui du son. La courbe qui en ressort est continue et représente fidèlement le mouvement du son dans les moindres détails et dans la continuité du temps. Entre chaque point de cette courbe, il y a des points à l’infini – comme dans le cas d’une ligne droite continue : la courbe est donc « dense », dans le sens mathématique du mot, c’est-à-dire qu’elle ne comporte aucun saut. (Vitali-Rosati 2014)

{{< hint essai >}}

*Exemple*: Pour représenter le temps et son défilement, une montre analogique indique l'heure à l'aide du déplacement des aiguilles sur un cadran. Ce qui veut dire que les aiguilles « font comme si » elles incarnaient le déplacement du temps, les rouages de l'horlogerie sont une analogie du mouvement du temps (si forte d'ailleurs que passées dans l'imaginaire).

{{< /hint >}}

<---> 

##### Numérique

{{< hint info >}}

*Définition*: Le modèle numérique représente la réalité par des nombres (le 0 et le 1 parce qu'entre ces deux valeurs se trouve une infinité de variables 0,1 ; 0,01 ; 0,001 etc.). Il considère la donnée comme discrète donc divisible donc le signal numérique est discontinu comme un échantillon.

{{< /hint >}}

> Le principe du numérique est de discrétiser le continu du son -- ou de l’image ou de n’importe quelle autre information. Cette discrétisation est ce que l’on appelle « échantillonnage ». Concrètement, on prend le continu de l’onde sonore et on choisit des échantillons, à savoir on ne considère pas l’ensemble du son, mais seulement les changements qui se produisent à des intervalles déterminés. Plus court est l’intervalle choisi, plus précis sera l’échantillonnage, et plus haute sera la qualité du son numérisé. Le son que l’on obtient de cette manière est essentiellement de qualité inférieure à l’analogique, car il ne rend pas compte de la continuité du son d’origine, mais seulement d’un nombre restreint -- bien qu’élevé -- d’échantillons. Mais le processus de discrétisation permet une simplification de l’enregistrement qui est réduit à une série de chiffres entiers et plus précisément de 0 et de 1. (Vitali-Rosati 2014)

{{< hint essai >}}

*Exemple*: Pour représenter le temps et son défilement, une montre numérique (dépourvue d'aiguilles) affiche l'heure sous forme de chiffres.

{{< /hint >}}

{{< /columns >}}

On peut appréhender la différence entre les deux modèles comme suit : 

> Prenez une route. 
>
> Cette route, vous avez envie de la connaître et de comprendre comment elle fonctionne.
>
> Face à cette route, votre question est donc : Comment puis-je connaître cette route dans son ensemble alors que moi humain je ne peux que me situer à un endroit précis, à un instant T sur cette route ? 
>
> Mes solutions sont les suivantes : 
>
> 1. je peux la connaître directement en la parcourant de tout son long ;
> 2. je peux la connaître indirectement en la représentant. 
>
> Si j'en fais un modèle (parce que j'ai la flemme de marcher) : 
>
> 1. je peux prendre la route comme une ligne continue qui va d'un point A à un point B = Modèle analogique ; 
> 2. je peux décider de diviser la route en portion entre A et B, de la *discrétiser* = Modèle numérique. 

![](https://mmellet.github.io/Enseignement-FRA3826_2023/images/analogique.jpg)

La différence entre numérique et analogique dépend donc du traitement de l'information ou du *signal* mais également du modèle qui est implémenté dans le dispositif. 

### La modèlisation

Le numérique est une approche que l'on peut voir émerger dans des artefacts plus anciens. Les architectures numériques dont nous héritons aujourd'hui ont été déterminées par un temps long d'élaboration. Cette première perspective sur le numérique nous permet de le comprendre au-delà d'une immanence technique mais aussi d'une révolution absolue. Souvent -- et c’est le cas de l’informatique, il y a eu une réflexion théorique avant d’y avoir une invention technologique. L'idée principale qui permet de comprendre le lien entre numérique et analogique et leur différence précédemment évoquée est le principe de modélisation. 

Pour exemplifier cette idée, l'un des plus anciens mécanismes, appelé la Machine d'Anticythère : 

![](https://mmellet.github.io/Enseignement-FRA3826_2023/images/mecanism.jpg)

Machine étonnante datée d'avant antérieure à 87 av. J.-C dont seulement trois fragments ont été conservées, le mécanisme d'Anticythère est considéré, après un travail de reconstitution de sa globalité effectué par les archéologues, comme le premier calculateur analogique. Ce mécanisme devait permettre de calculer la positions des astres, de situer par exemple la lune, de prévoir les éclipses etc. Selon les hypothèses, il fonctionnait par un système d'engrenages et des principes mathématiques issus des travaux de Pythagore pour reproduire le rythme des phases lunaires et des périodes synodiques (temps que met une planète pour revenir à la configuration Terre-planète-Soleil). Si les prédictions du mécanisme d'Anticythère devaient en réalité s'avérer pour la plupart fausses (les calcules étaient fondés sur une logique géocentrique) et qu'il demeure un calculateur analogique, l'important est ici de comprendre que toute technologie se fonde sur un principe de modélisation.

Toute représentation d'une donnée ou d'une réalité complexe se fonde sur un modèle : il y a donc eu modélisation avant implémentation. 

{{< hint essai >}}

*Exemple*: Derrière le dispositif horloger, il y a un modèle du temps (qui n'est pas le même que celui du cadran solaire), du temps comme calculable (divisible en heures, minutes, secondes mais aussi additionnable [60 minutes = 1 heure, 24 heures = une journée]). Le temps n'est ni dans la montre ni sur son cadran, qu'il soit par affichage analogique ou numérique, il est en dehors des dispositifs de représentation. 

{{< /hint >}}

La modélisation est donc proposer un système pour représenter une réalité en y apposant des valeurs, des règles qui nous semblent définir cette réalité. Le modèle numérique qui est celui de nos machines se fonde sur un principe de binarité. 

### Binarité

Avant d'être une affaire de mathématicien ou d'informaticien, le numérique est d'abord du code binaire, ou *langage machine*, ou *code machine* (Kittler 2015). Dans les machines numériques, tous les types de médias sont codés de façon binaire. Comme une montre qui ne connaît par ce qu'est le temps, le programme ne connaît pas les images, les sons ou même les lettres… Ce qu'il sait faire en revanche c'est manipuler que des 0 et des 1 et il les manipule toujours avec les mêmes règles. Même si l'on distingue les médias au sein d'une machine, entre texte, image, son ou autre, les données demeurent identiques, elles sont constituées de nombres. 

La différenciation des médias n'émane pas de leur encodage, la binarité reste la même pour tous, mais d'un traitement pour la lecture humaine produite par le *programme*. L'encodage des médias et de leurs contenus se fonde sur la modélisation en amont mais est aussi une histoire de langage. Le code binaire n'est pas prévu pour être écrit, édité ou même lu directement par l'humain : les langages de programmation en revanche fonctionnent comme des intermédiaires entre la machine et l'individu. 

{{< hint info >}}

*Définition*: En informatique, un langage de programmation est une notation conventionnelle destinée à formuler des algorithmes et produire des programmes informatiques qui les appliquent. D'une manière similaire à une langue naturelle, un langage de programmation est composé d'un alphabet, d'un vocabulaire, de règles de grammaire et de significations.

{{< /hint >}}

Les langages de programmation, couche qui nous permet d'éditer le rapport à la machine, sont autant de médiation qui peuvent distancier l'humain de son outil de travail comme le déplore Kittler : 

> [...] les codes commencèrent à croître de manière anarchique. La surface en argent de la puce, à la fois modèle et champ principal d’application de toutes les optimisations topologiques, perd la transparence mathématique qui fut la sienne : elle devient une tour de Babel, dans laquelle les décombres des tours déjà démolies restent intégrés.

La multiplication des langages intermédiaires du modèle numérique signe pour Kittler un éloignement intellectuel mais également très technique et épistémologique à la machine sur laquelle nous écrivons. S'ajoute à cette perspective, la question de la politique des formats, outils ou langages que j'utilise : ce que j'utilise pour structurer mon discours, à qui cela appartient-il ? qui en la maîtrise ou la propriété ? (questions pour la séance 5)

### L'indexation 

Le deuxième exemple qui permet d'inscrire le numérique au-delà d'un immanence technique et dans un temps pré-numérique est celui du *Mundaneum* de Paul Otlet et Henri La Fontaine. Le *Mundaneum* est un centre d'archives et fait se rejoindre deux projets importants qui sont conjoints : *L'Office international de bibliographie* (OIB) et *Le Répertoire bibliographique universel* (RBU) créés en 1895 par Otlet et La Fontaine. 

L'idée principales de ces architectures est celle de réunir et de classer l'ensemble de la connaissance humaine : l'OIB (qui deviendra un Institut) se fonde sur RBU qui rassemble depuis sa création 18 millions de fiches bibliographiques. 

On retrouve dans le travail de Otlet et La Fontaine l'idéal de la bibliothèque universelle (qui a été évoqué pour le projet Gutenberg) :

> And to travel along the entire library, it would take twice as many years as there are trillions of volumes in the library, which makes, as said before, the number 1 with 1,999,982 zeros. What I’m trying to explain is that we have just as little hope of imagining the number of years it takes light to travel along the library, as we have of imagining the number of volumes itself. And that really shows most clearly that the task of forming for oneself an image of this number is futile, even though the number is finite. (Lasswitz, *Die Universalbibliothek*, 1904, traduction de Born)

> Et pour parcourir toute la bibliothèque, il faudrait deux fois plus d'années qu'il n'y a de trillions de volumes dans la bibliothèque, ce qui fait, comme je l'ai déjà dit, le chiffre 1 avec 1 999 982 zéros. Ce que j'essaie d'expliquer, c'est que nous avons aussi peu d'espoir d'imaginer le nombre d'années qu'il faut à la lumière pour parcourir la bibliothèque, que d'imaginer le nombre de volumes lui-même. Et cela montre bien que la tâche de se faire une image de ce nombre est vaine, même si ce nombre est fini. (*Même extrait* qui inspirera plus tard la *Bibliothèque de Babel* de Borges, 1941).

La conviction d'Otlet et de La Fontaine est que la connaissance et sa large diffusion constituent des puissances pacifistes : pour résoudre les conflits mondiaux, il faut diffuser en réseau la connaissance qui aura été indexée au préalable. 

> Rassembler et classer les savoirs du monde relève d’un projet global qui se construit dans le contexte spécifique des mouvements pour la paix, de la fondation de la Société des Nations (SDN) et des deux guerres mondiales. (Schafer, « Le Mundaneum, un patrimoine inclassable », 155)

La Fontaine était spécialiste en droit international et impliqué dans de nombreuses associations pour la défense des droits humains (président du Bureau International de la Paix depuis 1907). 

Les travaux d'Otlet et La Fontaine préfigurent déjà une architecture d'Internet son principe d'indexation : le Mundaneum constitue le projet d’archivage documentaire universel. Le Répertoire n'avait pas pour fonction de devenir une bibliothèque physique mais de posséder la description et l'information du lieu de conservation de tous les ouvrages sous toutes les formes (livres, affiches, journaux) sous la forme d'une fiche. Les demandes d'information adressées nécessitaient une recherche longue et minutieuse dans le catalogue de dossier, recherche effectuée par les secrétaires du projet avant que les principes des navigateurs et des agrégateurs de contenu n'existent. Cette modélisation papier d'une connaissance humaine navigable est à l’origine de Google ou de Wikipédia : l'indexation d'information qui permet d'établir un lieu unique où sont centralisées les connaissances. 

![Le Répertoire Bibliographique Universel](https://mmellet.github.io/Enseignement-FRA3826_2023/images/mundaneum.jpg)

Le *Mundaneum* tel qu'il était prévu, donc avec une diffusion mondiale est en fait l'anticipation des réseaux d'informations numériques, du réseau qui s'imposera (à savoir Internet), à l'exception d'un élément : l'aspect géopolitique. Dans toute son idéalité pacifiste, la projection d'Otlet n'a pas anticipé les disparités globales, le développement du réseau principalement dans l'hémisphère nord, et plus largement la monopolisation du réseau par les puissances mondiales. 

![« Le Pont mondial », Paul Otlet, 1937. Collections de la Fédération Wallonie Bruxelles de Belgique, mises en dépôt au Mundaneum, centre d’archives, à Mons.](https://mmellet.github.io/Enseignement-FRA3826_2023/images/repertoire.jpg)


### L'appréhension

Avec l'idée que l'édition est toujours une appréhension très concrète des caractéristiques physiques et plastiques du support d'inscription, nous partirons du principe que le numérique s'appréhende comme une matière, donc non comme « virtuel » au sens commun du terme. Le terme virtuel parce qu'il est employé comme synonyme d'immatériel ou de dématérialisé est problématique lorsque l'on parle d'édition numérique (et de numérique en général) parce qu'il nous ramène à une abstraction de l'information et de la connaissance, et donc, par extension, à un déracinement, de l'importance de la structuration éditoriale des données. Penser que dans les environnements numériques, l'information ou la connaissance se *dématérialise* implique que le travail d'édition se résume à un travail visuel vague, qui n'a donc plus d'inscription et d'impact concret sur le support d'inscription. 

L'abstraction est à ce point importante et inscrite dans les disciplines en sciences humaines qu'elle en devient une tradition : les textes, l'écrit, la pensée, le fait littéraire sont des objets d'étude abordés largement hors de leur espace d'incarnation, ce qui est reconduit avec encore plus de force dans la culture numérique par un imaginaire filé de la liquidité : au point de devenir iconique (Brown 1989), la métaphore liquide définit l'ensemble du lexique numérique (navigateurs, ancres, *phishing* ou hameçonnage, pirate, *streaming* etc.). La considération du numérique plus proche de l'éther (*cloud* ou nuage) que d'un enracinement fait de l'environnement d'édition une infrastructure vide, flottante et impalpable mais surtout insaisissable (par la pensée) par défaut. 

Si ce premier imaginaire éloigne déjà ce que peut être un geste éditorial d'une conscience technique de son impact et de son fonctionement, les théories *du sentiment* s'ajoutent encore au tableau. Soit technophobe, soit technophile, l'appréhension du support numérique par le sentiment mène à de la terreur (dans le premier cas) ou du sublime (dans le deuxième) tel que défini par Olivier Dyiens (Dyens 2019). 

{{< hint essai >}}

*Exemple*: Dyens donne pour illustrer la théorie de la Terreur et du Sublime l'exemple d'AlphaGo, un programme conçu pour jouer au jeu de Go (jeu proche des échecs avec plus de complexité de déplacements, c'est-à-dire qu'à chaque tour, le joueur doit décider entre 200 possibilités de déplacement [contre 35 aux échecs]). AlphaGo a été programmé à partir de 30 millions de mouvements (ce qui est loin de représenter toutes les possibilités de jeu de Go) pour qu'à partir de ces mouvements, il puisse élaborer des stratégies. Pour tester le programme AlphaGo, on le fait combattre le champion du jeu de Go, à savoir Lee Sedol. Ce qu'analyse Dyens est la réaction que provoque le comportement d'AlphaGo lors du tournoi : non seulement il gagne les premières parties, mais il développe surtout des mouvements qui ne font pas partie de son corpus d'apprentissage. 

{{< /hint >}}

Face à AlphaGo, comme face à une machine qui marche, le sentiment humaine, nous dit Dyens, oscille entre une forme de crainte et une fascination, entre Terreur et Sublime. De ces deux émotions, Olivier Dyens en parle comme « la saveur fondamentale de notre réel [...] l'expérience quotidienne de la distance entre les mondes », entre les mondes c'est à dire entre ce que je crois être possible et ce que la machine m'impose lorsqu'elle me prouve le contraire. 

La critique que l'on peut opposer à cette théorie du sentiment, même si elle a des intérêts évidents pour comprendre l'impact culturel des développements numériques et d'un bouleversement des frontières entre humain et non-humain, c'est que ni la Terreur ni le Sublime ne peuvent nous aider à comprendre comment AlphaGo génère des mouvements, comment une machine produit son propre déplacement, ou comment un programme quelconque établit une édition. Tout l'intérêt du modèle technique en amont de ces actions se trouve dans le brume de réactions qui, si elles demeurent légitimes, n'amènent pas vers la connaissance du lieu où s'opère une édition numérique. Terreur ou Sublime ramènent à l'idée d'une étrangeté radicale, ce qui coupe littéralement la compréhension : si les environnements numériques me paraissent étrangers à mes conceptions ou mes exceptions, ils ont cependant été pensés et prévus pour mon usage (donc ils me concernent) mais surtout de manière plus fondamentale, ils sont tels qu'ils sont parce que la culture du temps de leur émergence et développement est telle qu'elle est.  

La connaissance de l'impact de l'édition numérique ne peut donc être appréhendée s'il demeure une forme de sacralisation ou abstraction d'une pensée et de sa matière : pour comprendre comment un livre précis a été édité, il faut l'observer, parfois le démonter, le manipuler, le tester[^raison].

[^raison]: C'est notamment pour cette raison que le cours comprend quelques exercices pratiques. 

### La déliaison

Cette appréhension très concrète permet également de ramener une dimension importante de l'édition, d'autant plus importante pour des contenus multimédias : la dimension sensorielle. Si l'on aime l'odeur des vieux livres oubliés des étagères et que l'on se désole des liseuses froides et inodores, c'est bien que notre approche du support comprend une part sensorielle : comment se retranscrit la part sensorielle d'une édition numérique ? 

Il est évident que les matières utilisées dans un ordinateur ne vont pas sentir un papier vieilli, ce qui ne veut pas impliquer que l'on est complètement du sensoriel pour autant : dans l'édition numérique autant que dans la tradition imprimée, tout passe par l'empreinte et le toucher/frapper des touches du clavier. L'écran en tant que tel qui est cette interface lumineuse qui permet de montrer et d'afficher les résultats de nos manipulations sur la machine est une source de lumière, tandis que le corps de la machine est une source de chaleur : à la différence d'un livre qui ne « chauffe » pas, votre machine, plus elle est utilisée sur un temps long et tenu, vous tient de radiateur. 

> La  dimension  sensible  de  la  lecture  et  ses  effets  ont  souvent amené  les  historiens  à survaloriser  l’exclusivité  du  livre  imprimé.  S’il  est  vrai  que  tenir  entre  ses  mains, «toucher»  un  livre  joue  un  rôle  important  dans  l’appréciation  de  l’objet,  il  est  tout aussi vrai que la navigation et le feuilletage numériques ont leurs plaisirs spécifiques, leurs esthétiques propres. (Doueihi, 2009, p. 111).

L'usure est notamment un indicateur de la matérialité de l'environnement numérique d'écriture et d'édition : 

> [...] la vitre des écrans se brisent, les écrans tactiles se graissent, les touches du clavier s'érodent (d'ailleurs on voit ainsi quelles sont les touches les plus utilisées), les boîtiers peuvent se fendrent, l'aération de la machine peine et c'est ce qui fait que des vieux ordinateurs font des bruits d'avion lorsqu'ils démarrent, etc. (Bonaccorsi, 2012)

Cette usure est causée comme pour toute matière qui est un outil, par la série de manipulations, de gestes, de transports, et, de la même manière qu'une page écornée tourne toujours, donc fonctionne, un écran griffé affiche toujours. 

La différence majeure dans le rapport matériel, qui lie autant le sentiment d'étrangeté d'une sensorialité que l'imaginaire d'une virtualité des architectures numériques, est une logique de déliaison entre la trace et le support : 

>[la trace numérique] n’est pas nécessairement intentionnelle […] [elle] est volatile, en quelque sorte déliée de l’émetteur et de ses supports ; elle est essentiellement dérive, dissémination, décontextualisation. (Petit et Bouchardon 2017)

Comme le dit également Merzeau (2009), les traces de l'écriture numérique, parce qu'elles sont à distance de leurs sources, sont dissociables non seulement de leur lieu d'inscription mais également des gestes de production : si mon écran se brise, l'écriture de mon fichier, ou l'édition de la page Web que j'étais en train de consulter ne sont pas impactées ; en revanche, si le serveur ou le « nuage » qui contient mes données est détruit, je ne pourrai plus y avoir accès et ce, même si mon écran est intact. 

> Le propre du numérique est que le contenu ne porte pas sur lui les traces de sa manipulation – et pourtant ces traces existent. (Merzeau 2009)

Si le support papier est orthothétique, parce que l’écriture sur ce suppport rend compte des traces de sa manipulation ou, pour reprendre l’expression de Stiegler de « ce qui est pensé comme étant ce qui s’est passé » ; le numérique en revanche est autothétique, c’est-à-dire qu’il porte en lui-même sa propre réalité, sa propre thèse. Ainsi l’écriture numérique ne rend pas compte de ce qui est pensé comme étant ce qui s’est passé, elle ne porte pas sa propre édition. Comme le souligne d’ailleurs Merzeau « les données numériques ne dépendent plus de la stabilité d’un support » -- mais elles dépendent encore d’une matérialité et d'une infrastructure bien particulière.

## Internet 

*Rappel*: Internet n'est **pas** le Web qui n'est pas l'informatique. Internet est une infrastructure et le Web désigne le contenu de cette infrastructure donc *Internet est la route où le Web circule*. 

L'édition numérique se conçoit comme une édition désormais en réseau donc sur l'idée d'une circulation des informations : le principe de réseau tel qu'il est désormais implémenté dans notre culture numérique n'est cependant pas neutre ou, pour le dire autrement, Internet, normalisé comme le réseau principal du flux d'informations, est loin d'être exempt de déterminations politiques, économiques ou plus largement culturelles. 

{{< hint info >}}

*Définition*: Internet est une forme de télécommunication (*communication de loin*) comme la route ou la poste mais multidirectionnel (à la différence des télécommunications monodirectionnelles comme la radio ou encore bidirectionnelle comme le téléphone). Internet est né dans les années 1960, et renvoie au réseau informatique mondial : c’est une infrastructure qui relie des ordinateurs du monde entier, permettant ainsi l'échange d'informations. Internet est donc un réseau permettant à d’autres réseaux (comme le Web) d’exister. 

{{< /hint >}}


### Arpanet

Paul Baran, un informaticien, physicien et mathématicien américain, publie en 1964 ses premiers résultats sur la communication de données par paquets, première recherche sur la communication sur réseau de données par paquets. La communication dite par paquets signifie que les messages envoyés sont divisés, ou *discrétisés*, en petits paquets pour permettre un envoi et une circulation plus rapides. Baran travaille à l'époque dans une agence de sécurité américaine et propose au Pentagone le projet d'un réseau militaire distribué. Projet qui ne sera pas réalisé -- abandonné en 1965 -- mais qui est aux origines du projet Arpanet.

Les principes du projet de Baran seront repris par Robert Taylor, directeur de l'IPTO, le Bureau des Technologies de Traitement de l'Information au sein de l'ARPA, agence de recherche technologique du département de la Défense des États-Unis. Taylor relance l'idée d'un réseau en février 1966 avec pour objectifs d'améliorer la communication scientifique entre des chercheurs en informatique au sein d'ARPA (les ARPA's Contractors). L'idée de Taylor est en réalité projet de celle d'Otlet et La Fontaine : pour permettre de plus grande avancée humaine en terme de développements des connaissances (et pour Taylor, de développements en terme de défense militaire des États-Unis), il faut donner aux chercheurs des outils qui leur permettent d'échanger les informations plus rapidement et plus loin. 

> Nous allons construire un réseau et vous allez y participer. Et vous allez connecter vos machines. En vertu de cela, nous allons réduire nos besoins en informatique. (*Annonce d'avril 1967 d'un des responsable du projet Larry Roberts*) 

Arpanet est construit grâce à des financements militaires et connecte les 4 universités sélectionnées : l’UCLA (Université de Californie à Los Angeles), le SRI (Stanford Research Institute) à Stanford, l’UCSB (l’Université de Californie à Santa Barbara) et l’Université d'Utah. La première connexion a lieu en 1969. Si ce réseau est encore à ses premiers balbutiements, il représente un changement en terme de communication par qu'Arpanet permet des échanges entre différentes machines, soit des machines de constructeurs différents. 

![](https://mmellet.github.io/Enseignement-FRA3826_2023/images/arpanet.png)

Bien que les premiers utilisateurs d'Arpanet aient été des chercheurs universitaires, il ne faut pas minimiser ni le militarisme ni l'américanisme de ce réseau à l'origine d'Internet : dès les années 1950, les recherches militaires ont en tête la constitution d'un système de communication qui puisse résister à des attaques physiques et notamment à une guerre nucléaire. Arpanet incarne cet horizon en ce qu'il fonctionne par noeuds interconnectés : même si la connexion est perdue entre UCLA et UCSB, les autres communications demeurent fonctionnelles. 

### Protocoles

L'Internet tel que nous le connaissons apparaît à la suite de la création de deux protocoles en 1970, protocoles de communication qui vont devenir des standards :

{{< details title="Le Protocole IP" open=false >}}

{{< hint info >}}

*Définition*: Le Protocole IP (*Internet Protocol*) attribue une adresse unique ou une IP et fournit des indications pour faire passer ces données dans les réseaux. 

{{< /hint >}}

{{< hint essai >}}

*Exemple*: L'adresse IP 192. 0. 31. 10 permet aussi d'identifier un ordinateur sur Internet.

{{< /hint >}}

{{< /details >}}

{{< details title="Le Protocole TCP" open=false >}}

{{< hint info >}}

*Définition*: Le Protocole TCP (*Transmission Control Protocol*) permet d'établir la connexion entre les deux machines. C'est ce qui permet de garder la trace d'un échange, c'est ce qui vous informe notamment lorsque le message n'est pas arrivé à destination. Comme les paquets IP sont limités à un équivalent de 200 mots, le protocole TCP se charge de découper l'ensemble des paquets et de les réassembler à l'arrivée. Il est neutre dans la mesure où il n'interprête pas les données, il les délivre comme un postier.

{{< /hint >}}

{{< /details >}}

Cette avancée sur la transmission des paquets depuis les travaux de Baran fait émerger une période d'essor des réseaux (ou *inter-networking*) avec la multiplication de réseaux informatiques : AlohaNet à l’Université d’Hawaii, PRNet (Packet Radio Net) réseau par radio, le Minitel français en 1978, Ethernet en 1980 etc.

### De l'Arpa à l'Inter

En 1983, après qu'Arpanet adopte finalement les deux protocoles de communication qui se sont imposés comme des standards, il est progressivement assimilé au réseau par défaut, jusqu'à être désigné par le terme au départ générique d'*Internet*. Parce qu'il permet d'interconnecter différentes machines mais aussi également différents réseaux, Arpanet/Internet s'impose comme le « réseau des réseaux ». L'explosion d'Arpanet le fait se scinder en deux branches : une branche militaire MILNET / et / une branche civile, universitaire et scientifique qui est connue comme l'Internet.

C'est au début des années 1990, que se développe l'aspect le plus connu sur Internet aujourd'hui : le Web. 

## Le Web 

Internet est donc ce réseau permettant le transfert de données entre machines. Ce transfert s'opère au travers d'un maillage de serveurs, mais il est déjà parcouru par des données et des interconnections : le Web. À l'origine, le Web désigne les hyperliens entre les pages html, non les connections au réseau.

### TBL 

![Robert Cailliau](https://mmellet.github.io/Enseignement-FRA3826_2023/images/cailliau.jpg)

Le Web c'est le premier serveur à utiliser le procole HTTP (*HyperText Transfer Protocol*) dévelopé par Tim Berners-Lee et Robert Caillau dans le cadre d'un projet du CERN (Suisse). L'objectif était de développer un protocole capable de gérer les hypertexte (ce qui était impossible avec les protocoles TCP/IP) dans la continuité du projet hypertexte. Appelé aussi la Toile, le web est la partie la plus visible d'Internet, c’est un service parmi d’autres d’Internet. Le Web renvoie en fait au système hypertexte public qui permet de naviguer de page en page en cliquant sur des liens grâce à un navigateur. Il correspond à l'ensemble des pages publiques reliées à l'aide d'hyperliens. Ce système de liaisons entre les pages est à la base du Web et fait graphiquement penser à une toile d'araignée (ou *web* en anglais). Le Web est donc une technologie qui a créé ses propres bases selon des enjeux scientifiques (et littéraires), son propre protocole, soit son propre langage, et qui est aujourd'hui est devenu un modèle du numérique notamment en s'articulant à deux autres standards : 

- l'URL pour pouvoir identifier toute ressource dans un hyperlien ;
- le langage HTML pour écrire des pages web contenant des hyperliens.

À partir de 1994, le Web connaît une explosion en parallèle à l’apparition de NCSA Mosaic, un navigateur développé par Eric Bina et Marc Andreessen au *National Center for Supercomputing Applications* (NCSA), dans l’Illinois. Le navigateur Mosaic est le précuseur de nos navigateurs dans la mesure où il pose les bases de l'interface graphique de nos navigateurs modernes (intégration des images au texte). Le Web se développera par la suite dans d'autres navigateurs (Google, Firefox, Apache) et s'imposer comme standard. 

### Générations Web

On distingue plusieurs générations de Web : 


{{< tabs "uniqueid" >}}
{{< tab "Web 1.0" >}} un web statique soit qui est un web de lecture : il n'y a pas d'interaction possible avec le contenu hyperlien. {{< /tab >}}
{{< tab "Web 2.0" >}} web qui n'est plus un web statique parce qu'il repose sur des systèmes architecturaux qui permettent une utilisation intelligente responsive et donc la participation des utilisateurs. Il est alors possible de créer le contenu d'une page et de l'éditer en temps réel. C'est ce que l'on appelle le Web social ou Web collaboratif. Ce principe, on le retrouve dans Wikipédia (que l'on verre plus en détail dans une séance prochaine), mais également dans d'autres plateformes comme Facebook ou Youtude. Les utilisateurs deviennent ainsi des créateurs de contenus et participent à l'élaboration de ce contenu.{{< /tab >}}
{{< tab "Web 3.0" >}} Web sémantique ou Web des objets -- développement toujours en cours qui ne fait pas l'unanimité pour des questions autant philosophiques qu'éthiques. Le Web 3.0 développe cette idée d'un web personnalisé soit une page du Web 3.0 est créée automatiquement dans le but d'être adaptée à chaque utilisateur et à son environnement. Ainsi le Web 3.0 se créé au fur à mesure des utilisations.{{< /tab >}}
{{< /tabs >}}

### Web alternatif

Dernier point à préciser sur le Web : sa dimension alternative et cachée. Selon le navigateur, selon les connexions, selon le profil d'utilisateur ou utilisatrice, nous n'avons pas accès tous et toutes aux mêmes informations : qu'il s'agisse de l'organisation et l'ordre des pages lorsque l'on effectue ou une requête ou même de la nature des réponses par le moteur de recherche, il y a une sélection qui a été faite. 

À cette différence de traitements selon les utilisateurs s'ajoute le fait que tout le Web, malgré les aspirations d'Otlet et La Fontaine, n'est pas indexé : ce qui veut dire qu'une immense partie des contenus circulant sur Internet ne sont pas accessibles via nos navigateurs. Selon une étude de 2001, la partie « cachée » du Web représente plus de 90% de ses contenus. Ces ressources invisibles peuvent autant être des pages administratives payantes, des pages hors du protocole HTTP, des ressources incompatibles avec les moteurs de recherche, ou des ressources qui ne seront tout simplement pas jugées comme pertinentes pour votre requête par les moteurs de recherche. 
 
Ce que l'on appelle le Web profond désigne donc cette partie du Web qui n’est pas indexée et donc introuvable avec les moteurs de recherche généralistes : elle ne doit pas être confondue avec le plus célèbre *dark* ou *deep* Web qui désigne le Web clandestin, uniquement accessible via des configurations particulières, des autorisations spéciales et qui charrie un certain nombre de contenus marginaux ou illégaux. 

## La Machine

Thématique qui sera appronfondie dans la séance suivante, on peut déjà établir la machine, devenue l'outil principal des gestes d'édition, ainsi : 

{{< hint info >}}

*Définition*: L'ordinateur est un système de traitement de l'information programmable et donc une sort de super-éditeur.

{{< /hint >}}

> Machine algorithmique composée d'un assemblage de matériels correspondant à des fonctions spécifiques, capable de recevoir de l'information, dotée de mémoires à grande capacité et de moyens de traitement à grande vitesse, pouvant restituer tout ou partie des éléments traités, ayant la possibilité de résoudre des problèmes mathématiques et logiques complexes, et nécessitant pour son fonctionnement la mise en oeuvre et l'exploitation automatique d'un ensemble de programmes enregistrés. (TLFI)

S'il a été pensé à l'origine comme une calculatrice puissante pour résoudre des calculs complexes, le principe de l'ordinateur est plus ancien (Babbage, Lovelace jusqu'à Turing qui en donnera une théorie formelle). Concrètement, l'ordinateur se situe au centre de la production éditoriale : 

- machine d’écriture (production) ;
- de stockage, de transmission des textes (diffusion) ;
- de lecture et de publicisation des textes (légitimation).

Et pourtant c'est un outil que nous (en tant que sciences humaines) connaissons peu, avec l'idée que ses méandres soit relèvent d'une boîte noire, soit ne relèvent pas de nos domaines d'études ou d'expertises. Dans des séances suivantes, on se posera la question de savoir à quel point l'ordinateur peut aider l'éditeur jusqu'à s'y substituer. 

## Édition électronique, numérique, en réseau

Comme il y a eu l’invention de l’écriture, puis du codex, puis de l’imprimerie (inventions qui sont loin d'être uniques et exclusives au monde occidental), le XX<sup>e</sup> siècle a vu naître une nouvelle technique, qui bouleverse nos habitudes et nos modèles : il s’agit de l’informatique -- qui, avec l’émergence d’Internet puis du Web, a peu à peu évolué vers ce que l’on appelle l’ère numérique.

Si on réfléchit à l'informatique, dans le cadre d'une histoire de l'édition et des technologies de l'édition, on est arrivé à une phase qui prolonge le projet de l'imprimé comme on a pu le voir dans la séance précédente, à savoir l'automatisation du processus de production des livres et de publication qui va impliquer : 
- l'émergence de nouveaux formats pour la diffusion des textes ou d'autres contenus :
- la diffusion très large et libre (investissement sur le Web) ;
- la question de la légitimation des statuts d'auteur et d'éditeur (que l'on verra dans les séances suivantes).

### Édition par strates

Si la distinction entre édition électronique, numérique ou en réseau (on a également parlé d'édition *digitale*) apparaît dans l'article de Dacos et Mounier, elle est aujourd'hui datée mais représente un socle important de ce que l'on considère aujourd'hui plus généralement par édition numérique. 

{{< hint warning >}}

*Exposé*: Inscrivant leur réflexion dans l'actualité d'une angoisse de disparition dont on connaît désormais les ressorts platoniciens de la disparition d'une culture par un changement de pratiques et de supports d'inscription, et d'une crainte du remplacement de l'intermédiaire (qui n'est pas sans évoquer les craintes de Kittler vis-à-vis des programmes informatiques), Dacos et Mounier distinguent édition électronique, numérique et en réseau comme suit : 

- l'édition électronique est un premier temps qui concerne la numérisation de textes appartenant auparavant à d'autres supports, des supports analogiques ;
- l'édition numérique concerne les textes nativement numérique ;
- l'édition en réseau désigne un dernier temps de l'édition électronique et représente l'ensemble des pratiques de communication au sein d'Internet (partage de document, écriture collective et collaborative).

{{< /hint >}}

Dans cette vision stratifiée (qui est dite non-chronologique mais qui semble être appréhendée sous bien des aspects comme telle), il y a une sorte d'idéalité qui ne correspond pas totalement à la réalité des littératures numériques dont les auteurs sont conscients puisqu'ils parlent eux-mêmes de « sédimentation » entre les étapes : le projet Gutenberg par exemple déjà en 1971 comporte les trois temps électronique (numérisation de textes imprimés mais édition sous de nouveaux formats, partage des textes en réseau). 

On peut mettre la définition de Dacos et Mounier en perspective en prenant en compte d'autres définitions dont celle proposée par Jean-Claude Guédon qui nous ramène également à une définition de l'édition : 

>L'édition électronique, ce n'est ni plus ni moins que le contenu des réseaux électroniques. En effet, éditer c'est faire paraître, c'est publier. Publier, c'est rendre public, c'est mettre un message dans une forme telle que l'on puisse y avoir accès à certaines conditions. Tout fichier numérisé est donc susceptible de jouer un rôle de communication (texte, image, son, etc.) (*L'édition savante et l'autoroute électronique*) 

Décider des conditions d'accessibilité, « de ce qui va être lu », ce point soulevé dans la séance précédente perdure dans l'approche de Guédon qui ne fait pas tant la distinction entre textes nativement ou non-nativement numériques qu'il prend l'édition dans un sens large et le lien qui se fait en fait entre les étapes (même sédimentées) de Dacos et Mounier relèvent justement de cet objectif de publication : le point commun est essentiellement de penser une structuration en vue d'une accessibilité. 

L'autre élément que l'on peut trouver qui vient sédimenter encore les catégories éditoriales est celui déjà abordé de la détermination de l'information par le geste et le travail éditorial : 

> L’illusion référentielle est en effet courante qui consiste à penser la numérisation comme une action technique neutre, le simple portage d’un support à l’autre. Cette opération repose en réalité sur un travail de représentation, et donc d’interprétation, de la source. Les choix interprétatifs qui sont faits sont déterminants dès la première photographie de la source au moyen d’un scanner, mais surtout lorsqu’il s’agit de définir l’unité documentaire à partir de laquelle le corpus sera ordonné. (Dacos et Mounier)

{{< hint warning >}}

*Commentaire*: Ce qui est dit ici sur le cas de l'édition électronique est en réalité vrai pour les autres étapes : en amont des remédiations (un texte imprimé vers un texte numérique), des productions nativement numériques ou des diffusion ont été décidé des modèles qui impliquent une approche particulière du contenu, de son importance et sa valeur culturelle, de son mode de lecture, de son accessibilité et même de ce qu'est l'accessibilité en tant que telle. Si les formes de l'édition numérique reposent comme le disent les auteurs sur une série de standards et causent de nouveaux paradigmes économiques, politiques et scientifiques (ce qui permet de répondre par l'affirmative à la question d'accroche posée par l'article), elles s'établissent également sur des modèles qui sont des appréhensions culturelles. 

{{< /hint >}}

Le point qui n'est pas soulevé par l'article, auquel la postérité des études répondent, est celui de la pérennité des dénominations « édition électronique, numérique, en réseau ». Si l'article fonde son propos, sa structure et même une partie de sa rhétorique sur la distinction entre des noms et des formules (tout en admettant que les éditions se recoupent, se recouvrent), il ne parle que peu de ce qu'il advient au nom d'édition lorsque ses strates sont sédimentées. Élément qui n'est pas anodin pour un article dont justement l'édition souffre littéralement de petits problèmes de maintient (vous aurez remarqué que des « ? » remplacent les espaces insécables du texte, ce qui le rend bien interrogateur).

### Une révolution numérique ?

Élément de conclusion pour la séance tout en l'articulant à l'article de Dacos et Mounier, la dimension « révolutionnaire » de l'édition numérique qu'il est intéressant d'interroger, tout comme il est important de dépasser nos sentiments de Terreur et de Sublime. Le terme de « révolution », employé à de multiples reprises dans l'article, est également un poncif du discours contemporain :

- comme une amélioration avec l'extension des limites ;
- comme un processus qui implique la disparition supposée de plusieurs marqueurs traditionnels (le statut et la nature de la lecture ; le rôle de l'éditeur et son expertise etc.)

Le changement des environnements numériques est bien évidemment culturel (Doueihi), ce qui implique qu'il est en partie épistémologique : il s'agit moins d'une disparition d'un mode par un autre qu'une évolution d'un mode (ou, pour proposer une image, il s'agit peut-être moins de disparition ou d'extinction que de mue et de métamorphose).

Si cela est peu mentionné et développé dans le fil de l'article, au-delà des phénomènes de rupture qu'amène le développement d'une édition numérique (en tant qu'économie, technique, institution et culture), il y a des effets de continuité (les structurations héritées du Codex, comme l'index, les notes de bas de page constituent toujours un modèle classique de l'édition de pages numériques même si ce sont désormais des hypertextes et hyperliens qui les gèrent) et des surtout des marqueurs maintenus (nous sommes toujours dans une culture de l'écriture). 

## [Bibliographie](https://www.zotero.org/groups/5124082/fra3826-a2023/collections/LWFIA6G5)
